{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rjoshi2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rjoshi2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rjoshi2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# This file will be used to prepare the data for the graphical models (or strategy structure predictions)\n",
    "import pickle, pdb\n",
    "import sys\n",
    "sys.path.append('..') # for utils\n",
    "sys.path.append('../..') # for cocoa\n",
    "import utils\n",
    "from collections import defaultdict as ddict\n",
    "import numpy as np\n",
    "from cocoa_folder.cocoa.core.dataset import Example\n",
    "from cocoa_folder.craigslistbargain.parse_dialogue import parse_example\n",
    "from cocoa_folder.craigslistbargain.core.price_tracker import PriceTracker\n",
    "from cocoa_folder.craigslistbargain.model.generator import Templates\n",
    "from cocoa_folder.craigslistbargain.core.scenario import Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_tracker = PriceTracker(utils.PROJ_DIR + \"cocoa_folder/craigslistbargain/price_tracker.pkl\")\n",
    "templates = Templates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w_strategies = pickle.load(open(utils.PROJ_DIR + 'data/negotiation_data/data_w_strategies_outcomes.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL has normalize_price = False\n",
    "# FULL_Yiheng has normalize_price = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_price = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strategies(data_point):\n",
    "    '''\n",
    "    Takes data point and returns list of negotiation strategies, and dialogue acts\n",
    "    '''\n",
    "    utterances = parse_example(Example.from_dict(data_point, Scenario), price_tracker, templates) # Taken from Yiheng's work\n",
    "    utterances = utterances[2:] # skip first two\n",
    "    #agent_list = [x['agent'] for x in data_point['events'] if type(x['data']) == str]\n",
    "    agent_list = [-1]\n",
    "    strategy_list = [['<start>']]    # start + len(events) #  +[x['strategies'] for x in data_point['events'] if type(x['data']) == str]\n",
    "    utterance_list = ['<start>'] # start + len(events)\n",
    "    dial_act_list = ['<start>']    # start + len(events)\n",
    "    last_strat = []\n",
    "    for i, utt in enumerate(utterances):\n",
    "        if utt.text == None:\n",
    "            text = '<' + str(data_point['events'][i]['action']) + '>' # offer, accept, reject, quit\n",
    "            agent = data_point['events'][i]['agent']\n",
    "            dial_act = '<' + utt.lf.to_dict()['intent'] + '>'\n",
    "            if dial_act != '<start>': strats = last_strat             # copy last strategy for end portions\n",
    "            else:                     strats = []\n",
    "        else:\n",
    "            text = utils.normalizeString(utt.text, utt.lf.to_dict(), data_point[\"scenario\"], normalize_price = norm_price)\n",
    "            strats = data_point['events'][i]['strategies']\n",
    "            last_strat = strats\n",
    "            agent = data_point['events'][i]['agent']\n",
    "            dial_act = utt.lf.to_dict()['intent']\n",
    "        utterance_list.append(text)\n",
    "        dial_act_list.append(dial_act)\n",
    "        agent_list.append(agent)\n",
    "        strategy_list.append(strats)\n",
    "    assert len(utterance_list) == len(data_point['events']) + 1, pdb.set_trace()\n",
    "    assert len(utterance_list) == len(strategy_list), pdb.set_trace()\n",
    "    assert len(utterance_list) == len(agent_list), pdb.set_trace()\n",
    "    assert len(utterance_list) == len(dial_act_list), pdb.set_trace()\n",
    "    return agent_list, utterance_list, strategy_list, dial_act_list\n",
    "    \n",
    "#    return [(x['agent'], x['data'], x['strategies']) for x in data_point['events'] if type(x['data']) == str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtype :  train\n",
      "Dtype :  test\n",
      "Dtype :  dev\n"
     ]
    }
   ],
   "source": [
    "# full_data = [] # list of conversations\n",
    "# for i, data_point in enumerate(train_data):\n",
    "#     agent_list, utterance_list, strategies_list, dial_act_list = get_strategies(data_point)\n",
    "#     ratio = utils.getRatio(data_point)\n",
    "#     full_data.append({'agent_list': agent_list,\n",
    "#                      'utterance': utterance_list,\n",
    "#                      'strategies': strategies_list,\n",
    "#                      'ratio': ratio,\n",
    "#                      'dial_acts': dial_act_list\n",
    "#                      })\n",
    "# print (len(full_data))\n",
    "# print (full_data[76])\n",
    "\n",
    "full_data_all = {}\n",
    "for dtype in data_w_strategies:\n",
    "    print ('Dtype : ', dtype)\n",
    "    data = data_w_strategies[dtype]\n",
    "    full_data_list = []\n",
    "    for i, data_point in enumerate(data):\n",
    "        agent_list, utterance_list, strategies_list, dial_act_list = get_strategies(data_point)\n",
    "        ratio = utils.getRatio(data_point)\n",
    "        uuid  = data_point['uuid']\n",
    "        scene = (data_point[\"scenario\"][\"kbs\"][1][\"item\"][\"Category\"] + \" \" + \" \".join(data_point[\"scenario\"][\"kbs\"][1][\"item\"][\"Description\"])+ \" \" + data_point[\"scenario\"][\"kbs\"][1][\"item\"][\"Title\"])\n",
    "        scene = utils.normalizeString(scene, {\"price\":None}, data_point[\"scenario\"], normalize_price = norm_price) \n",
    "        full_data_list.append({'agent_list': agent_list,\n",
    "                     'utterance': utterance_list,\n",
    "                     'strategies': strategies_list,\n",
    "                     'ratio': ratio,\n",
    "                     'dial_acts': dial_act_list,\n",
    "                     'uuid': uuid,\n",
    "                     'scene': scene,\n",
    "                     'raw_data': data_point\n",
    "                     })\n",
    "    #print (len(full_data_list))\n",
    "    #print (full_data_list[76])\n",
    "    full_data_all[dtype] = full_data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only use full_data_all from now on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTYPE :  train\n",
      "defaultdict(<class 'int'>, {'<start>': 5383, 'buyer_pos_sentiment': 12710, 'first_person_singular_count_buyer': 14341, 'third_person_singular_buyer': 8122, 'buyer_propose': 5249, 'hedge_count_buyer': 7023, 'assertive_count_buyer': 2044, 'politeness_buyer_greet': 3596, 'first_person_singular_count_seller': 11780, 'third_person_singular_seller': 8878, 'number_of_diff_dic_pos': 18610, 'number_of_diff_dic_neg': 10402, 'personal_concern_seller': 9135, 'liwc_certainty': 2530, 'seller_propose': 3200, 'hedge_count_seller': 5204, 'assertive_count_seller': 2393, 'first_person_plural_count_seller': 1843, 'politeness_seller_greet': 3043, 'factive_count_buyer': 1666, 'factive_count_seller': 1763, 'third_person_plural_buyer': 744, 'seller_neg_sentiment': 2587, 'seller_pos_sentiment': 12152, 'liwc_informal': 2396, 'who_propose': 1009, 'first_person_plural_count_buyer': 1033, 'politeness_seller_gratitude': 1073, 'buyer_neg_sentiment': 1093, 'politeness_seller_please': 60, 'politeness_buyer_gratitude': 2098, 'third_person_plural_seller': 977, 'seller_trade_in': 883, 'politeness_seller_please_s': 77, 'family': 201, 'politeness_buyer_please': 188, 'friend': 149, 'politeness_buyer_please_s': 47})\n",
      "{'assertive_count_buyer', 'third_person_plural_buyer', 'family', 'factive_count_seller', 'politeness_seller_please_s', 'buyer_pos_sentiment', 'first_person_plural_count_seller', 'hedge_count_buyer', 'assertive_count_seller', 'friend', 'third_person_plural_seller', 'third_person_singular_seller', 'factive_count_buyer', 'politeness_buyer_please_s', 'liwc_certainty', '<start>', 'number_of_diff_dic_pos', 'first_person_plural_count_buyer', 'hedge_count_seller', 'seller_propose', 'seller_neg_sentiment', 'politeness_seller_gratitude', 'liwc_informal', 'politeness_seller_please', 'first_person_singular_count_seller', 'seller_trade_in', 'politeness_buyer_please', 'third_person_singular_buyer', 'buyer_neg_sentiment', 'politeness_buyer_greet', 'politeness_seller_greet', 'who_propose', 'buyer_propose', 'politeness_buyer_gratitude', 'first_person_singular_count_buyer', 'number_of_diff_dic_neg', 'seller_pos_sentiment', 'personal_concern_seller'}\n",
      "Total 38 strategies\n",
      "defaultdict(<class 'int'>, {'<start>': 5383, 'init-price': 4828, 'unknown': 9838, 'insist': 474, 'counter-price': 8042, 'agree': 1985, '<offer>': 4657, '<accept>': 4084, 'intro': 4743, 'inquiry': 5224, 'disagree': 2085, 'vague-price': 401, 'inform': 2413, '<quit>': 474, '<reject>': 349})\n",
      "{'inform', 'init-price', 'inquiry', 'disagree', 'counter-price', 'unknown', '<reject>', '<offer>', 'intro', 'vague-price', '<start>', '<accept>', '<quit>', 'insist', 'agree'}\n",
      "Total 15 dial acts\n",
      "DTYPE :  test\n",
      "defaultdict(<class 'int'>, {'<start>': 656, 'buyer_pos_sentiment': 1157, 'seller_pos_sentiment': 1143, 'third_person_singular_seller': 812, 'number_of_diff_dic_pos': 2016, 'first_person_singular_count_buyer': 572, 'third_person_singular_buyer': 893, 'buyer_propose': 613, 'hedge_count_buyer': 710, 'number_of_diff_dic_neg': 1144, 'factive_count_buyer': 193, 'assertive_count_buyer': 217, 'buyer_neg_sentiment': 113, 'politeness_buyer_please': 19, 'politeness_buyer_greet': 66, 'seller_neg_sentiment': 292, 'seller_propose': 347, 'first_person_singular_count_seller': 370, 'politeness_buyer_gratitude': 107, 'personal_concern_seller': 967, 'friend': 17, 'assertive_count_seller': 251, 'third_person_plural_seller': 115, 'who_propose': 119, 'first_person_plural_count_seller': 159, 'hedge_count_seller': 533, 'factive_count_seller': 176, 'liwc_certainty': 254, 'seller_trade_in': 93, 'first_person_plural_count_buyer': 105, 'politeness_seller_gratitude': 62, 'liwc_informal': 85, 'politeness_seller_greet': 59, 'politeness_seller_please': 6, 'third_person_plural_buyer': 85, 'family': 23})\n",
      "{'assertive_count_buyer', 'third_person_plural_buyer', 'family', 'factive_count_seller', 'buyer_pos_sentiment', 'first_person_plural_count_seller', 'hedge_count_buyer', 'assertive_count_seller', 'friend', 'third_person_plural_seller', 'third_person_singular_seller', 'personal_concern_seller', 'factive_count_buyer', 'liwc_certainty', '<start>', 'number_of_diff_dic_pos', 'first_person_plural_count_buyer', 'hedge_count_seller', 'seller_propose', 'seller_neg_sentiment', 'politeness_seller_gratitude', 'liwc_informal', 'politeness_seller_please', 'first_person_singular_count_seller', 'politeness_buyer_please', 'seller_trade_in', 'third_person_singular_buyer', 'buyer_neg_sentiment', 'who_propose', 'politeness_seller_greet', 'buyer_propose', 'politeness_buyer_gratitude', 'first_person_singular_count_buyer', 'number_of_diff_dic_neg', 'seller_pos_sentiment', 'politeness_buyer_greet'}\n",
      "Total 36 strategies\n",
      "defaultdict(<class 'int'>, {'<start>': 656, 'intro': 573, 'unknown': 1155, 'init-price': 561, 'counter-price': 916, 'agree': 217, '<offer>': 546, '<accept>': 474, 'inquiry': 610, 'inform': 274, 'vague-price': 36, '<reject>': 50, 'disagree': 244, 'insist': 48, '<quit>': 67})\n",
      "{'insist', 'inform', 'init-price', 'inquiry', 'disagree', 'unknown', '<reject>', '<offer>', 'intro', 'vague-price', '<start>', '<accept>', '<quit>', 'counter-price', 'agree'}\n",
      "Total 15 dial acts\n",
      "DTYPE :  dev\n",
      "defaultdict(<class 'int'>, {'<start>': 643, 'politeness_buyer_greet': 435, 'seller_pos_sentiment': 1354, 'number_of_diff_dic_pos': 2172, 'seller_neg_sentiment': 318, 'first_person_singular_count_seller': 1346, 'number_of_diff_dic_neg': 1227, 'buyer_neg_sentiment': 144, 'factive_count_buyer': 174, 'third_person_singular_seller': 974, 'hedge_count_seller': 570, 'first_person_singular_count_buyer': 1660, 'third_person_singular_buyer': 984, 'buyer_propose': 609, 'personal_concern_seller': 1053, 'politeness_seller_greet': 364, 'hedge_count_buyer': 838, 'assertive_count_seller': 270, 'buyer_pos_sentiment': 1511, 'liwc_informal': 254, 'first_person_plural_count_buyer': 117, 'first_person_plural_count_seller': 193, 'liwc_certainty': 294, 'seller_propose': 373, 'who_propose': 121, 'politeness_buyer_gratitude': 231, 'third_person_plural_buyer': 90, 'factive_count_seller': 201, 'seller_trade_in': 105, 'assertive_count_buyer': 239, 'third_person_plural_seller': 117, 'politeness_seller_gratitude': 119, 'politeness_seller_please_s': 13, 'politeness_buyer_please': 23, 'politeness_buyer_please_s': 7, 'family': 30, 'politeness_seller_please': 9, 'friend': 18})\n",
      "{'assertive_count_buyer', 'third_person_plural_buyer', 'family', 'factive_count_seller', 'politeness_seller_please_s', 'buyer_pos_sentiment', 'first_person_plural_count_seller', 'hedge_count_buyer', 'assertive_count_seller', 'friend', 'third_person_plural_seller', 'third_person_singular_seller', 'factive_count_buyer', 'politeness_buyer_please_s', 'liwc_certainty', '<start>', 'number_of_diff_dic_pos', 'first_person_plural_count_buyer', 'hedge_count_seller', 'seller_propose', 'seller_neg_sentiment', 'politeness_seller_gratitude', 'liwc_informal', 'politeness_seller_please', 'first_person_singular_count_seller', 'seller_trade_in', 'politeness_buyer_please', 'third_person_singular_buyer', 'buyer_neg_sentiment', 'politeness_buyer_greet', 'politeness_seller_greet', 'who_propose', 'buyer_propose', 'politeness_buyer_gratitude', 'first_person_singular_count_buyer', 'number_of_diff_dic_neg', 'seller_pos_sentiment', 'personal_concern_seller'}\n",
      "Total 38 strategies\n",
      "defaultdict(<class 'int'>, {'<start>': 643, 'intro': 552, 'unknown': 1167, 'disagree': 256, 'init-price': 542, '<offer>': 557, '<accept>': 467, '<reject>': 65, 'inquiry': 625, 'inform': 283, 'counter-price': 945, 'agree': 232, '<quit>': 57, 'vague-price': 54, 'insist': 39})\n",
      "{'insist', 'inform', 'init-price', 'disagree', 'inquiry', 'counter-price', 'unknown', '<offer>', 'intro', 'vague-price', '<start>', '<accept>', '<quit>', '<reject>', 'agree'}\n",
      "Total 15 dial acts\n"
     ]
    }
   ],
   "source": [
    "# See Frequent strategies\n",
    "strategies_freq = {}\n",
    "all_strategies = {}\n",
    "dial_act_freq = {}\n",
    "all_dial_act = {}\n",
    "for dtype in full_data_all:\n",
    "    print ('DTYPE : ', dtype)\n",
    "    data = full_data_all[dtype]\n",
    "    strategies_freq_ = ddict(int)\n",
    "    dial_act_freq_   = ddict(int)\n",
    "    all_strategies_ = set()\n",
    "    all_dial_act_   = set()\n",
    "    for i, data_point in enumerate(data):\n",
    "        for conv in data_point['strategies']:\n",
    "            for strategy in conv:\n",
    "                strategies_freq_[strategy] += 1\n",
    "                all_strategies_.add(strategy)\n",
    "        for da in data_point['dial_acts']:\n",
    "            dial_act_freq_[da] += 1\n",
    "            all_dial_act_.add(da)\n",
    "    print (strategies_freq_)\n",
    "    print (all_strategies_)\n",
    "    print (f'Total {len(all_strategies_)} strategies')\n",
    "    print (dial_act_freq_)\n",
    "    print (all_dial_act_)\n",
    "    print (f'Total {len(all_dial_act_)} dial acts')\n",
    "    strategies_freq[dtype] = strategies_freq_\n",
    "    all_strategies[dtype]  = all_strategies_\n",
    "    dial_act_freq[dtype] = dial_act_freq_\n",
    "    all_dial_act[dtype]  = all_dial_act_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FEATURE MAP IS USED BY YIHENG\n",
    "recommendation_feature_mapping = {\"seller_neg_sentiment\":0,\"seller_pos_sentiment\":1,\n",
    "                                  \"buyer_neg_sentiment\":2,\"buyer_pos_sentiment\":3,\n",
    "                                  \"first_person_plural_count_seller\":4,\"first_person_singular_count_seller\":5,\n",
    "                                  \"first_person_plural_count_buyer\":6,\"first_person_singular_count_buyer\":7,\n",
    "                                  \"third_person_singular_seller\":8,\"third_person_plural_seller\":9,\n",
    "                                  \"third_person_singular_buyer\":10,\"third_person_plural_buyer\":11,\n",
    "                                  \"number_of_diff_dic_pos\":12,\"number_of_diff_dic_neg\":13,\n",
    "                                  \"buyer_propose\":14,\"seller_propose\":15,\n",
    "                                  \"hedge_count_seller\":16,\"hedge_count_buyer\":17,\n",
    "                                  \"assertive_count_seller\":18,\"assertive_count_buyer\":19,\n",
    "                                  \"factive_count_seller\":20,\"factive_count_buyer\":21,\n",
    "                                  \"who_propose\":22,\"seller_trade_in\":23,\n",
    "                                  \"personal_concern_seller\":24,\"sg_concern\":25,\n",
    "                                  \"liwc_certainty\":26,\"liwc_informal\":27,\n",
    "                                  \"politeness_seller_please\":28,\"politeness_seller_gratitude\":29,\n",
    "                                  \"politeness_seller_please_s\":30,\n",
    "                                  \"ap_des\":31,\"ap_pata\":32,\"ap_infer\":33,\n",
    "                                  \"family\":34,\"friend\":35,\n",
    "                                  \"politeness_buyer_please\":36,\"politeness_buyer_gratitude\":37,\n",
    "                                  \"politeness_buyer_please_s\":38,\n",
    "                                  \"politeness_seller_greet\":39,\"politeness_buyer_greet\":40}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "{'<start>'}\n",
      "{'ap_pata', 'ap_infer', 'sg_concern', 'ap_des'}\n",
      "test\n",
      "{'<start>'}\n",
      "{'politeness_buyer_please_s', 'sg_concern', 'ap_infer', 'politeness_seller_please_s', 'ap_des', 'ap_pata'}\n",
      "dev\n",
      "{'<start>'}\n",
      "{'ap_pata', 'ap_infer', 'sg_concern', 'ap_des'}\n"
     ]
    }
   ],
   "source": [
    "# See what all strategies are there in the feature mapping and not there in all types of data\n",
    "yiheng_strategies = set([k for k in recommendation_feature_mapping])\n",
    "for dtype in all_strategies:\n",
    "    print (dtype)\n",
    "    all_strategies_ = all_strategies[dtype]\n",
    "    print (all_strategies_ - yiheng_strategies)\n",
    "    print (yiheng_strategies - all_strategies_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seller_neg_sentiment': 'neg_sentiment', 'seller_pos_sentiment': 'pos_sentiment', 'buyer_neg_sentiment': 'neg_sentiment', 'buyer_pos_sentiment': 'pos_sentiment', 'first_person_plural_count_seller': 'first_person_plural_count', 'first_person_singular_count_seller': 'first_person_singular_count', 'first_person_plural_count_buyer': 'first_person_plural_count', 'first_person_singular_count_buyer': 'first_person_singular_count', 'third_person_singular_seller': 'third_person_singular', 'third_person_plural_seller': 'third_person_plural', 'third_person_singular_buyer': 'third_person_singular', 'third_person_plural_buyer': 'third_person_plural', 'number_of_diff_dic_pos': 'number_of_diff_dic_pos', 'number_of_diff_dic_neg': 'number_of_diff_dic_neg', 'buyer_propose': 'propose', 'seller_propose': 'propose', 'hedge_count_seller': 'hedge_count', 'hedge_count_buyer': 'hedge_count', 'assertive_count_seller': 'assertive_count', 'assertive_count_buyer': 'assertive_count', 'factive_count_seller': 'factive_count', 'factive_count_buyer': 'factive_count', 'who_propose': 'who_propose', 'seller_trade_in': 'trade_in', 'personal_concern_seller': 'personal_concern', 'sg_concern': 'sg_concern', 'liwc_certainty': 'liwc_certainty', 'liwc_informal': 'liwc_informal', 'politeness_seller_please': 'politeness_please', 'politeness_seller_gratitude': 'politeness_gratitude', 'politeness_seller_please_s': 'politeness_please', 'ap_des': 'ap_des', 'ap_pata': 'ap_pata', 'ap_infer': 'ap_infer', 'family': 'family', 'friend': 'friend', 'politeness_buyer_please': 'politeness_please', 'politeness_buyer_gratitude': 'politeness_gratitude', 'politeness_buyer_please_s': 'politeness_please', 'politeness_seller_greet': 'politeness_greet', 'politeness_buyer_greet': 'politeness_greet'}\n"
     ]
    }
   ],
   "source": [
    "# Map recommendation feature mapping to a uniform_strategy_mapping\n",
    "# This uniform mapping is without sellers or buyers\n",
    "# For exampel : {buyer_propose : propose, seller_propose : propose}\n",
    "recommended2uniform_strategymapping = {}\n",
    "idx = 0\n",
    "for strategy in recommendation_feature_mapping:\n",
    "    new_strategy = strategy.replace('_seller', '').replace('_buyer', '').replace('seller_', '').replace('buyer_', '').replace('please_s', 'please')\n",
    "    recommended2uniform_strategymapping[strategy] = new_strategy\n",
    "print (recommended2uniform_strategymapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended2uniform_strategymapping['<start>'] = '<start>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seller_neg_sentiment': 'neg_sentiment',\n",
       " 'seller_pos_sentiment': 'pos_sentiment',\n",
       " 'buyer_neg_sentiment': 'neg_sentiment',\n",
       " 'buyer_pos_sentiment': 'pos_sentiment',\n",
       " 'first_person_plural_count_seller': 'first_person_plural_count',\n",
       " 'first_person_singular_count_seller': 'first_person_singular_count',\n",
       " 'first_person_plural_count_buyer': 'first_person_plural_count',\n",
       " 'first_person_singular_count_buyer': 'first_person_singular_count',\n",
       " 'third_person_singular_seller': 'third_person_singular',\n",
       " 'third_person_plural_seller': 'third_person_plural',\n",
       " 'third_person_singular_buyer': 'third_person_singular',\n",
       " 'third_person_plural_buyer': 'third_person_plural',\n",
       " 'number_of_diff_dic_pos': 'number_of_diff_dic_pos',\n",
       " 'number_of_diff_dic_neg': 'number_of_diff_dic_neg',\n",
       " 'buyer_propose': 'propose',\n",
       " 'seller_propose': 'propose',\n",
       " 'hedge_count_seller': 'hedge_count',\n",
       " 'hedge_count_buyer': 'hedge_count',\n",
       " 'assertive_count_seller': 'assertive_count',\n",
       " 'assertive_count_buyer': 'assertive_count',\n",
       " 'factive_count_seller': 'factive_count',\n",
       " 'factive_count_buyer': 'factive_count',\n",
       " 'who_propose': 'who_propose',\n",
       " 'seller_trade_in': 'trade_in',\n",
       " 'personal_concern_seller': 'personal_concern',\n",
       " 'sg_concern': 'sg_concern',\n",
       " 'liwc_certainty': 'liwc_certainty',\n",
       " 'liwc_informal': 'liwc_informal',\n",
       " 'politeness_seller_please': 'politeness_please',\n",
       " 'politeness_seller_gratitude': 'politeness_gratitude',\n",
       " 'politeness_seller_please_s': 'politeness_please',\n",
       " 'ap_des': 'ap_des',\n",
       " 'ap_pata': 'ap_pata',\n",
       " 'ap_infer': 'ap_infer',\n",
       " 'family': 'family',\n",
       " 'friend': 'friend',\n",
       " 'politeness_buyer_please': 'politeness_please',\n",
       " 'politeness_buyer_gratitude': 'politeness_gratitude',\n",
       " 'politeness_buyer_please_s': 'politeness_please',\n",
       " 'politeness_seller_greet': 'politeness_greet',\n",
       " 'politeness_buyer_greet': 'politeness_greet',\n",
       " '<start>': '<start>'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended2uniform_strategymapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# Unique types of uniform_strategies\n",
    "print (len(set([v for k,v in recommended2uniform_strategymapping.items()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip strategies :  ['who_propose', 'ap_pata', 'ap_infer', 'sg_concern', 'ap_des']\n",
      "{'family': 0, 'third_person_plural': 1, 'friend': 2, 'trade_in': 3, 'first_person_plural_count': 4, 'politeness_greet': 5, 'neg_sentiment': 6, 'politeness_gratitude': 7, 'propose': 8, 'hedge_count': 9, '<start>': 10, 'number_of_diff_dic_pos': 11, 'first_person_singular_count': 12, 'assertive_count': 13, 'liwc_informal': 14, 'pos_sentiment': 15, 'personal_concern': 16, 'third_person_singular': 17, 'politeness_please': 18, 'number_of_diff_dic_neg': 19, 'factive_count': 20, 'liwc_certainty': 21}\n"
     ]
    }
   ],
   "source": [
    "uniform_feature_mapping = {}\n",
    "skip_strategies = ['who_propose'] + list(yiheng_strategies - all_strategies['dev'])\n",
    "print ('Skip strategies : ', skip_strategies)\n",
    "uniform_strategies = set([v for k,v in recommended2uniform_strategymapping.items()])\n",
    "idx = 0\n",
    "for i, item in enumerate(uniform_strategies):\n",
    "    if item in skip_strategies:\n",
    "        continue\n",
    "    uniform_feature_mapping[item] = idx\n",
    "    idx += 1\n",
    "print (uniform_feature_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165682\n",
      "   first_person_singular_count 0.1577 26121\n",
      "                 pos_sentiment 0.1501 24862\n",
      "        number_of_diff_dic_pos 0.1123 18610\n",
      "         third_person_singular 0.1026 17000\n",
      "                   hedge_count 0.0738 12227\n",
      "        number_of_diff_dic_neg 0.0628 10402\n",
      "              personal_concern 0.0551 9135\n",
      "                       propose 0.0510 8449\n",
      "              politeness_greet 0.0401 6639\n",
      "                       <start> 0.0325 5383\n",
      "               assertive_count 0.0268 4437\n",
      "                 neg_sentiment 0.0222 3680\n",
      "                 factive_count 0.0207 3429\n",
      "          politeness_gratitude 0.0191 3171\n",
      "     first_person_plural_count 0.0174 2876\n",
      "                liwc_certainty 0.0153 2530\n",
      "                 liwc_informal 0.0145 2396\n",
      "           third_person_plural 0.0104 1721\n",
      "                   who_propose 0.0061 1009\n",
      "                      trade_in 0.0053 883\n",
      "             politeness_please 0.0022 372\n",
      "                        family 0.0012 201\n",
      "                        friend 0.0009 149\n",
      "'Total 22 strategies'\n"
     ]
    }
   ],
   "source": [
    "# See frequency of uniform_strategies\n",
    "strategies_freq = ddict(int)\n",
    "for i, data_point in enumerate(full_data_all['train']):\n",
    "    for conv in data_point['strategies']:\n",
    "        for strategy in conv:\n",
    "#             if strategy == '<start>':\n",
    "#                 uniform_strategy = strategy\n",
    "#             else:\n",
    "            uniform_strategy = recommended2uniform_strategymapping[strategy]\n",
    "            strategies_freq[uniform_strategy] += 1\n",
    "from pprint import pprint\n",
    "#pprint (sorted(strategies_freq.items(), key = lambda x: x[1], reverse = True))\n",
    "tot = np.sum([v for k,v in strategies_freq.items()])\n",
    "print (tot)\n",
    "for k,v in (sorted(strategies_freq.items(), key = lambda x: x[1], reverse = True)):\n",
    "    print (\"%30s %.4f %d\" % (k, v / tot, v))\n",
    "pprint (f'Total {len(strategies_freq)-1} strategies') # -1 for who_propose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54980\n",
      "                       unknown 0.1789 9838\n",
      "                 counter-price 0.1463 8042\n",
      "                       <start> 0.0979 5383\n",
      "                       inquiry 0.0950 5224\n",
      "                    init-price 0.0878 4828\n",
      "                         intro 0.0863 4743\n",
      "                       <offer> 0.0847 4657\n",
      "                      <accept> 0.0743 4084\n",
      "                        inform 0.0439 2413\n",
      "                      disagree 0.0379 2085\n",
      "                         agree 0.0361 1985\n",
      "                        insist 0.0086 474\n",
      "                        <quit> 0.0086 474\n",
      "                   vague-price 0.0073 401\n",
      "                      <reject> 0.0063 349\n",
      "'Total 15 dialogue acts'\n"
     ]
    }
   ],
   "source": [
    "# See frequency of dial_acts\n",
    "da_freq = dial_act_freq['train']\n",
    "from pprint import pprint\n",
    "#pprint (sorted(strategies_freq.items(), key = lambda x: x[1], reverse = True))\n",
    "tot = np.sum([v for k,v in da_freq.items()])\n",
    "print (tot)\n",
    "for k,v in (sorted(da_freq.items(), key = lambda x: x[1], reverse = True)):\n",
    "    print (\"%30s %.4f %d\" % (k, v / tot, v))\n",
    "pprint (f\"Total {len(all_dial_act['train'])} dialogue acts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'family': 0, 'third_person_plural': 1, 'friend': 2, 'trade_in': 3, 'first_person_plural_count': 4, 'politeness_greet': 5, 'neg_sentiment': 6, 'politeness_gratitude': 7, 'propose': 8, 'hedge_count': 9, 'number_of_diff_dic_pos': 10, 'first_person_singular_count': 11, 'assertive_count': 12, 'liwc_informal': 13, 'pos_sentiment': 14, 'personal_concern': 15, 'third_person_singular': 16, 'politeness_please': 17, 'number_of_diff_dic_neg': 18, 'factive_count': 19, 'liwc_certainty': 20, '<start>': 21}\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "chosen_strategies_mapping = {}#{k:v for k,v in uniform_feature_mapping.items() if k != '<start>'}\n",
    "cntr = 0\n",
    "for k,v in uniform_feature_mapping.items():\n",
    "    if k!='<start>':\n",
    "        chosen_strategies_mapping[k] = cntr\n",
    "        cntr += 1\n",
    "chosen_strategies_mapping['<start>'] = len(chosen_strategies_mapping)\n",
    "# skip_strategies = ['who_propose'] + list(yiheng_strategies - all_strategies)\n",
    "# print (\"Skip strategies : \", skip_strategies)\n",
    "# idx = 0\n",
    "# for strategy, mapping in uniform_feature_mapping.items():\n",
    "#     if strategy in skip_strategies:\n",
    "#         continue\n",
    "#     chosen_strategies_mapping[strategy] = idx\n",
    "#     idx += 1\n",
    "print (chosen_strategies_mapping)\n",
    "print (len(chosen_strategies_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inform': 0, 'init-price': 1, 'inquiry': 2, 'disagree': 3, 'counter-price': 4, 'unknown': 5, '<reject>': 6, '<offer>': 7, 'intro': 8, 'vague-price': 9, '<accept>': 10, '<quit>': 11, 'insist': 12, 'agree': 13, '<start>': 14}\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "chosen_dial_act_mapping = {}#{k:i for i, k in enumerate(all_dial_act['train']) if k != '<start>' }\n",
    "cntr = 0\n",
    "for k,v in enumerate(all_dial_act['train']):\n",
    "    if v != '<start>':\n",
    "        chosen_dial_act_mapping[v] = cntr\n",
    "        cntr += 1\n",
    "chosen_dial_act_mapping['<start>'] = len(chosen_dial_act_mapping)\n",
    "print (chosen_dial_act_mapping)\n",
    "print (len(chosen_dial_act_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ap_pata, ap_des, ap_infer and sg_concern are strategies that are learned using the classifiers. \n",
    "# Yiheng did not have any way to extract these for the data from the code he had access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "[(0, 3062), (1, 13552), (2, 10047), (3, 8840), (4, 7222), (5, 5025), (6, 3329), (7, 1958), (8, 1116), (9, 511), (10, 236), (11, 61), (12, 17), (13, 4)]\n",
      "Mean strategies per utt :  2.9951436886140415\n",
      "test\n",
      "[(0, 1059), (1, 1839), (2, 1160), (3, 892), (4, 635), (5, 367), (6, 251), (7, 134), (8, 49), (9, 29), (10, 9), (11, 3)]\n",
      "Mean strategies per utt :  2.2452154971215186\n",
      "dev\n",
      "[(0, 409), (1, 1646), (2, 1159), (3, 1031), (4, 845), (5, 578), (6, 356), (7, 223), (8, 143), (9, 51), (10, 26), (11, 12), (12, 5)]\n",
      "Mean strategies per utt :  2.942473781616286\n"
     ]
    }
   ],
   "source": [
    "# See # of strategies per utterance frequency to see distribution\n",
    "for dtype in full_data_all:\n",
    "    print (dtype)\n",
    "    strategies_perutt_freq = ddict(int)\n",
    "    for i, data_point in enumerate(full_data_all[dtype]):\n",
    "        for conv in data_point['strategies']:\n",
    "            num = len([c for c in conv if recommended2uniform_strategymapping[c] in chosen_strategies_mapping])\n",
    "            strategies_perutt_freq[num] += 1\n",
    "    print (sorted(strategies_perutt_freq.items(), key = lambda x: x))\n",
    "    tot = np.sum([v for k,v in strategies_perutt_freq.items()])\n",
    "    mean_strategies_per_turn = np.sum([k*v for k,v in strategies_perutt_freq.items()]) / tot\n",
    "    print ('Mean strategies per utt : ', mean_strategies_per_turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Max turns are 47\n",
      "Avg turns are 10.213635519227196\n",
      "[(3, 246), (4, 309), (5, 182), (6, 178), (7, 247), (8, 471), (9, 511), (10, 677), (11, 585), (12, 623), (13, 414), (14, 333), (15, 211), (16, 137), (17, 100), (18, 49), (19, 46), (20, 25), (21, 19), (22, 10), (23, 4), (24, 1), (25, 1), (26, 1), (30, 1), (31, 1), (47, 1)]\n",
      "test\n",
      "Max turns are 24\n",
      "Avg turns are 9.797256097560975\n",
      "[(3, 44), (4, 45), (5, 35), (6, 22), (7, 24), (8, 63), (9, 58), (10, 66), (11, 77), (12, 63), (13, 51), (14, 39), (15, 23), (16, 23), (17, 9), (18, 5), (19, 3), (20, 3), (21, 1), (24, 2)]\n",
      "dev\n",
      "Max turns are 28\n",
      "Avg turns are 10.08398133748056\n",
      "[(3, 35), (4, 47), (5, 21), (6, 24), (7, 25), (8, 65), (9, 57), (10, 66), (11, 66), (12, 64), (13, 48), (14, 53), (15, 21), (16, 21), (17, 10), (18, 9), (19, 6), (20, 1), (21, 1), (23, 1), (25, 1), (28, 1)]\n"
     ]
    }
   ],
   "source": [
    "# See max and avg turns\n",
    "for dtype in full_data_all:\n",
    "    print (dtype)\n",
    "    max_turns = 0\n",
    "    avg_turns = 0\n",
    "    turns_freq = ddict(int)\n",
    "    for i, data_point in enumerate(full_data_all[dtype]):\n",
    "        turns = len(data_point['strategies'])\n",
    "        max_turns = max(max_turns, turns)\n",
    "        avg_turns += turns\n",
    "        turns_freq[turns] += 1\n",
    "    avg_turns /= len(full_data_all[dtype])\n",
    "    print (f'Max turns are {max_turns}')\n",
    "    print (f'Avg turns are {avg_turns}')\n",
    "    print (sorted(turns_freq.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorify full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data into vector form\n",
    "def convert_2dlistofstrategies2vector(strategies, agent_list):\n",
    "    '''\n",
    "    Takes a list of list of strategies and returns numpy arr 2d numpy arr : conv_len x num_strats\n",
    "    '''\n",
    "    strats = np.zeros((len(strategies), len(chosen_strategies_mapping) + 1)) # 1 for agent\n",
    "    for i, stlist in enumerate(strategies):\n",
    "        for st in stlist:\n",
    "            uniform_st = recommended2uniform_strategymapping[st]\n",
    "            if uniform_st not in chosen_strategies_mapping:\n",
    "                continue\n",
    "            strats[i][chosen_strategies_mapping[uniform_st] + 1] = 1    # 1 for leaving space for ageent\n",
    "        try:\n",
    "            if agent_list[i] != -1:\n",
    "                strats[i][0] = agent_list[i]\n",
    "        except:\n",
    "            pdb.set_trace()\n",
    "    return np.array(strats)\n",
    "\n",
    "# for i, data_point in enumerate(full_data):\n",
    "#     full_data[i]['strategies_vec'] = convert_2dlistofstrategies2vector(full_data[i]['strategies'], full_data[i]['agent_list'])\n",
    "# print (full_data[1234])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test\n",
      "dev\n"
     ]
    }
   ],
   "source": [
    "for dtype in full_data_all:\n",
    "    print (dtype)\n",
    "    data = full_data_all[dtype]\n",
    "    for i in range(len(data)):\n",
    "        data[i]['strategies_vec'] = convert_2dlistofstrategies2vector(data[i]['strategies'], data[i]['agent_list'])\n",
    "        data[i]['dial_acts_vec']   = np.array([chosen_dial_act_mapping[d] for d in data[i]['dial_acts']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 23)\n",
      "(22,)\n"
     ]
    }
   ],
   "source": [
    "print (full_data_all['train'][1234]['strategies_vec'].shape)\n",
    "print (full_data_all['train'][1234]['dial_acts_vec'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_id': 0, 'family': 1, 'third_person_plural': 2, 'friend': 3, 'trade_in': 4, 'first_person_plural_count': 5, 'politeness_greet': 6, 'neg_sentiment': 7, 'politeness_gratitude': 8, 'propose': 9, 'hedge_count': 10, 'number_of_diff_dic_pos': 11, 'first_person_singular_count': 12, 'assertive_count': 13, 'liwc_informal': 14, 'pos_sentiment': 15, 'personal_concern': 16, 'third_person_singular': 17, 'politeness_please': 18, 'number_of_diff_dic_neg': 19, 'factive_count': 20, 'liwc_certainty': 21, '<start>': 22}\n",
      "{0: 'agent_id', 1: 'family', 2: 'third_person_plural', 3: 'friend', 4: 'trade_in', 5: 'first_person_plural_count', 6: 'politeness_greet', 7: 'neg_sentiment', 8: 'politeness_gratitude', 9: 'propose', 10: 'hedge_count', 11: 'number_of_diff_dic_pos', 12: 'first_person_singular_count', 13: 'assertive_count', 14: 'liwc_informal', 15: 'pos_sentiment', 16: 'personal_concern', 17: 'third_person_singular', 18: 'politeness_please', 19: 'number_of_diff_dic_neg', 20: 'factive_count', 21: 'liwc_certainty', 22: '<start>'}\n"
     ]
    }
   ],
   "source": [
    "strategies2colid = {}\n",
    "strategies2colid['agent_id'] = 0\n",
    "for s, idx in chosen_strategies_mapping.items():\n",
    "    strategies2colid[s] = idx+1 # for agent\n",
    "print (strategies2colid)\n",
    "colid2strategies = {v:k for k,v in strategies2colid.items()}\n",
    "print (colid2strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Originally had :  5383\n",
      "Taken          :  4828\n",
      "test\n",
      "Originally had :  656\n",
      "Taken          :  567\n",
      "dev\n",
      "Originally had :  643\n",
      "Taken          :  561\n"
     ]
    }
   ],
   "source": [
    "# Choose data based on certain characteristics\n",
    "# Drop conversations with < 4 utterances\n",
    "taken_data_all = {}\n",
    "for dtype in full_data_all:\n",
    "    print (dtype)\n",
    "    taken_data = []\n",
    "    for i in range(len(full_data_all[dtype])):\n",
    "        if len(full_data_all[dtype][i]['utterance']) <= 4:\n",
    "            continue\n",
    "        taken_data.append(full_data_all[dtype][i])\n",
    "    print ('Originally had : ', len(full_data_all[dtype]))\n",
    "    print ('Taken          : ', len(taken_data))\n",
    "    taken_data_all[dtype] = taken_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_vector_data = {'train': taken_data_all['train'], \n",
    "                        'strategies2colid': strategies2colid, \n",
    "                        'dialacts2id': chosen_dial_act_mapping,\n",
    "                        'test': taken_data_all['test'],\n",
    "                        'valid' : taken_data_all['dev'],}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total conversations in data and zeros among them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total conv are 67891. Out of that 3060 or 0.04507224816249576 are zero vectors.\n"
     ]
    }
   ],
   "source": [
    "total_conv = 0\n",
    "zero_strategy_conv = 0\n",
    "all_data_concatenated = full_data_all['train'] + full_data_all['test'] + full_data_all['dev'] \n",
    "for i in range(len(all_data_concatenated)):\n",
    "    total_conv += all_data_concatenated[i]['strategies_vec'].shape[0]\n",
    "    for j in range(all_data_concatenated[i]['strategies_vec'].shape[0]):\n",
    "        if all(all_data_concatenated[i]['strategies_vec'][j] == 0):\n",
    "            zero_strategy_conv += 1\n",
    "print (f'Total conv are {total_conv}. Out of that {zero_strategy_conv} or {zero_strategy_conv / total_conv} are zero vectors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Predict everyone as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Total is 1219138. Out of that 184512.0 are 1. Majority class would be : 0.8486537209077233.\n",
      "test\n",
      "Total is 140645. Out of that 16759.0 are 1. Majority class would be : 0.8808418358277934.\n",
      "dev\n",
      "Total is 142393. Out of that 21380.0 are 1. Majority class would be : 0.8498521696993532.\n"
     ]
    }
   ],
   "source": [
    "for dtype in taken_data_all:\n",
    "    print (dtype)\n",
    "    total = 0\n",
    "    ones = 0\n",
    "    for i in range(len(taken_data_all[dtype])):\n",
    "        temp = taken_data_all[dtype][i]['strategies_vec'].reshape(-1)\n",
    "        total += len(temp)\n",
    "        ones  += np.sum(temp)\n",
    "    print (f'Total is {total}. Out of that {ones} are 1. Majority class would be : {(total-ones)/total}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class weights (based on only training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7f10e6be9dd0>, {0: defaultdict(<class 'int'>, {1: 23908.0, 0: 29098.0}), 1: defaultdict(<class 'int'>, {1: 197.0, 0: 52809.0}), 2: defaultdict(<class 'int'>, {1: 1691.0, 0: 51315.0}), 3: defaultdict(<class 'int'>, {1: 146.0, 0: 52860.0}), 4: defaultdict(<class 'int'>, {1: 877.0, 0: 52129.0}), 5: defaultdict(<class 'int'>, {1: 2842.0, 0: 50164.0}), 6: defaultdict(<class 'int'>, {1: 6036.0, 0: 46970.0}), 7: defaultdict(<class 'int'>, {1: 3618.0, 0: 49388.0}), 8: defaultdict(<class 'int'>, {1: 3155.0, 0: 49851.0}), 9: defaultdict(<class 'int'>, {1: 8333.0, 0: 44673.0}), 10: defaultdict(<class 'int'>, {1: 11995.0, 0: 41011.0}), 11: defaultdict(<class 'int'>, {1: 18178.0, 0: 34828.0}), 12: defaultdict(<class 'int'>, {1: 25496.0, 0: 27510.0}), 13: defaultdict(<class 'int'>, {1: 4384.0, 0: 48622.0}), 14: defaultdict(<class 'int'>, {1: 2374.0, 0: 50632.0}), 15: defaultdict(<class 'int'>, {1: 24464.0, 0: 28542.0}), 16: defaultdict(<class 'int'>, {1: 8910.0, 0: 44096.0}), 17: defaultdict(<class 'int'>, {1: 16676.0, 0: 36330.0}), 18: defaultdict(<class 'int'>, {1: 357.0, 0: 52649.0}), 19: defaultdict(<class 'int'>, {1: 10191.0, 0: 42815.0}), 20: defaultdict(<class 'int'>, {1: 3370.0, 0: 49636.0}), 21: defaultdict(<class 'int'>, {1: 2486.0, 0: 50520.0}), 22: defaultdict(<class 'int'>, {1: 4828.0, 0: 48178.0})})\n"
     ]
    }
   ],
   "source": [
    "# num_zeros / num_ones for each class\n",
    "# sanity is that first class is 50/50\n",
    "num_feats = taken_data_all['train'][0]['strategies_vec'].shape[1]\n",
    "feat_freq = ddict(lambda : ddict(int))\n",
    "for i in range(len(taken_data_all['train'])):\n",
    "    vec = taken_data_all['train'][i]['strategies_vec']\n",
    "    for j in range(num_feats):\n",
    "        ones = np.sum(vec[:, j])\n",
    "        zeros = vec.shape[0] - ones\n",
    "        feat_freq[j][1] += ones\n",
    "        feat_freq[j][0] += zeros\n",
    "print (feat_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.2170821482349004, 1: 268.06598984771574, 2: 30.34594914251922, 3: 362.05479452054794, 4: 59.440136830102624, 5: 17.65095003518649, 6: 7.781643472498343, 7: 13.650635710337204, 8: 15.800633914421553, 9: 5.360974438977559, 10: 3.4190079199666528, 11: 1.9159423478930575, 12: 1.0789927831816755, 13: 11.090784671532846, 14: 21.32771693344566, 15: 1.166693917593198, 16: 4.949046015712683, 17: 2.1785799952026865, 18: 147.47619047619048, 19: 4.201256010205083, 20: 14.728783382789317, 21: 20.321802091713597, 22: 9.97887323943662}\n"
     ]
    }
   ],
   "source": [
    "feature_weights = {}\n",
    "for featid, freq in feat_freq.items():\n",
    "    ones = freq[1]\n",
    "    zeros = freq[0]\n",
    "    weight = zeros / ones\n",
    "    feature_weights[featid] = weight\n",
    "print (feature_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_vector_data['feature_weights'] = feature_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_dial_acts = len(taken_data_all['dialacts2id'])\n",
    "# da_freq = ddict(lambda: ddict(int)) # create again for chosen data\n",
    "# for i in range(len(taken_data_all['train'])):\n",
    "#     vec = taken_data_all['train'][i]['dial_acts_vec']\n",
    "\n",
    "## NO NEED TO CREATE FEATURE WEIGHTS FOR DIAL ACTS. MY TRAINER WILL DO IT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(strategy_vector_data, open('../../data/negotiation_data/strategy_vector_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump WFST DATA (seq of bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in ['train', 'valid', 'test']:\n",
    "    f = open(f'../wfst/finite_state_machine/seq_bag_strats_rj_{dtype}', 'w')\n",
    "    for dat in strategy_vector_data[dtype]:\n",
    "        for uttidx, svec in enumerate(dat['strategies_vec']):\n",
    "            f.write('<'+''.join([str(int(ch)) for ch in svec[1:]])+'> ')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump WFST DATA (end form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in ['train', 'valid', 'test']:\n",
    "    f = open(f'../wfst/finite_state_machine/seq_end_strats_rj_{dtype}', 'w')\n",
    "    for dat in strategy_vector_data[dtype]:\n",
    "        for uttidx, svec in enumerate(dat['strategies_vec']):\n",
    "            for colidx, ch in enumerate(svec[1:]):\n",
    "                if ch == 0: continue\n",
    "                strat = colid2strategies[colidx+1]\n",
    "                f.write('<'+strat+'> ')\n",
    "            f.write('<end> ')       # end of utt\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump WFST DATA (Dialogue Acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in ['train', 'valid', 'test']:\n",
    "    f = open(f'../wfst/finite_state_machine/seq_da_acts_rj_{dtype}', 'w')\n",
    "    for dat in strategy_vector_data[dtype]:\n",
    "        for uttidx, dial_act in enumerate(dat['dial_acts']):\n",
    "            f.write('<'+dial_act+'> ')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy Vector Data with WORD IDS\n",
    "\n",
    "## Have to bert ids, word ids (based on some pretrained embedding - see yiheng's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17418c3da7848cfaa4b72582ac8a836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = full_data_all.keys()\n",
    "word2id =  {\"[PAD]\": 0, \"<start>\":1,\"[CLS]\":2,\"[SEP]\":3,\"<offer>\":4,\"<reject>\":5,\"<accept>\":6,\"<quit>\":7,\"[UNK]\":8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size :  13119\n"
     ]
    }
   ],
   "source": [
    "# Set vocab using all training data\n",
    "total = len(word2id) # curr count\n",
    "for k, datapoint in enumerate(data_w_strategies['train']):\n",
    "    print (f'{k}/{len(data_w_strategies[\"train\"])}', end = '\\r')\n",
    "    scene = (datapoint[\"scenario\"][\"kbs\"][1][\"item\"][\"Category\"] + \" \" + \" \".join(datapoint[\"scenario\"][\"kbs\"][1][\"item\"][\"Description\"])+ \" \" + datapoint[\"scenario\"][\"kbs\"][1][\"item\"][\"Title\"])\n",
    "    scene = utils.normalizeString(scene, {\"price\":None}, datapoint[\"scenario\"], normalize_price=norm_price) \n",
    "    for word in scene.split():\n",
    "        if word not in word2id:\n",
    "            word2id[word] = total\n",
    "            total += 1\n",
    "            \n",
    "    utterances = parse_example(Example.from_dict(datapoint, Scenario), price_tracker, templates)[2:]\n",
    "    for i, uter in enumerate(datapoint['events']):\n",
    "        if uter['action'] != 'message': continue\n",
    "        tmp_dict = utterances[i]\n",
    "        for word in utils.normalizeString(uter[\"data\"], tmp_dict.lf.to_dict(), datapoint[\"scenario\"], normalize_price=norm_price).split():\n",
    "            if word not in word2id:\n",
    "                word2id[word] = total\n",
    "                total += 1\n",
    "print(\"Vocab size : \", len(word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_vector_data['word2id'] = word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'strategies2colid', 'dialacts2id', 'test', 'valid', 'feature_weights', 'word2id'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy_vector_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train\n",
      "4827/4828\n",
      "test\n",
      "566/567\n",
      "dev\n",
      "560/561\r"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize('[CLS]'))\n",
    "sep_token_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize('[SEP]'))\n",
    "for dtype in dtypes:\n",
    "    print ()\n",
    "    print (dtype)\n",
    "    if dtype == 'dev': dtype = 'valid'\n",
    "    for i, datapoint in enumerate(strategy_vector_data[dtype]):\n",
    "        print (f'{i}/{len(strategy_vector_data[dtype])}', end = '\\r')\n",
    "        toks_space = [[word2id['[CLS]']] + [word2id.get(w, word2id['[UNK]']) for w in utt.split(' ')] + [word2id['[SEP]']] for utt in datapoint['utterance']]\n",
    "        toks_bert  = [cls_token_id + tokenizer.convert_tokens_to_ids(tokenizer.tokenize(utt)) + sep_token_id for utt in datapoint['utterance']]\n",
    "        scene_space = [word2id.get(w, word2id['[UNK]']) for w in datapoint['scene'].split(' ')]\n",
    "        scene_bert  = cls_token_id + tokenizer.convert_tokens_to_ids(tokenizer.tokenize(datapoint['scene'])) + sep_token_id\n",
    "        datapoint['toks_space'] = toks_space\n",
    "        datapoint['toks_bert']  = toks_bert\n",
    "        datapoint['scene_space'] = scene_space\n",
    "        datapoint['scene_bert'] = scene_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356\n"
     ]
    }
   ],
   "source": [
    "# Max scene seq\n",
    "max_scene_seq = 0\n",
    "for dtype in dtypes:\n",
    "    if dtype == 'dev': dtype = 'valid'\n",
    "    for datapoint in strategy_vector_data[dtype]:\n",
    "        max_scene_seq = max(max_scene_seq, len(datapoint['scene_bert']))\n",
    "print (max_scene_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add ratio bucket (obtained from only train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4828\n"
     ]
    }
   ],
   "source": [
    "all_ratios = []\n",
    "for datapoint in strategy_vector_data['train']:\n",
    "    all_ratios.append(datapoint['ratio'])\n",
    "print (len(all_ratios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-99999 -0.2\n",
      "-0.2 0.16666666666666666\n",
      "0.16666666666666666 0.4\n",
      "0.4 0.6052631578947368\n",
      "0.6052631578947368 254.8139534883721\n"
     ]
    }
   ],
   "source": [
    "# Split into 5\n",
    "all_ratios = sorted(all_ratios)\n",
    "bucket_size = int(len(all_ratios) * 0.2)\n",
    "bucket1 = all_ratios[              : 1*bucket_size]\n",
    "bucket2 = all_ratios[1*bucket_size : 2*bucket_size]\n",
    "bucket3 = all_ratios[2*bucket_size : 3*bucket_size]\n",
    "bucket4 = all_ratios[3*bucket_size : 4*bucket_size]\n",
    "bucket5 = all_ratios[4*bucket_size :              ]\n",
    "print (min(bucket1), max(bucket1))\n",
    "print (min(bucket2), max(bucket2))\n",
    "print (min(bucket3), max(bucket3))\n",
    "print (min(bucket4), max(bucket4))\n",
    "print (min(bucket5), max(bucket5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test\n",
      "dev\n",
      "{'agent_list': [-1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0], 'utterance': ['<start>', 'i would love to buy', 'sure ! what s your price ?', 'im on a budget so i could do <price>_-0.1', 'how about <price>_-0.0 and i ll wave the deposit .', 'i will take it', 'great !', '<offer>', '<offer>', '<accept>', '<accept>'], 'strategies': [['<start>'], ['buyer_pos_sentiment', 'first_person_singular_count_buyer', 'hedge_count_buyer'], ['seller_pos_sentiment', 'number_of_diff_dic_neg', 'assertive_count_seller', 'personal_concern_seller'], ['first_person_singular_count_buyer', 'buyer_propose', 'hedge_count_buyer'], ['first_person_singular_count_seller', 'number_of_diff_dic_neg', 'hedge_count_seller', 'personal_concern_seller'], ['first_person_singular_count_buyer', 'third_person_singular_buyer'], ['seller_pos_sentiment'], ['seller_pos_sentiment'], ['seller_pos_sentiment'], ['seller_pos_sentiment'], ['seller_pos_sentiment']], 'ratio': 0.037037037037037035, 'dial_acts': ['<start>', 'intro', 'inquiry', 'init-price', 'counter-price', 'unknown', 'agree', '<offer>', '<offer>', '<accept>', '<accept>'], 'uuid': 'C_f82ff19f74094889873c8acc58ee582f', 'scene': 'housing this is a single family house in an excellent condition . it will be available for move in on july . it s in a very nice and quiet community with great school district . email me for viewing or further information . thanks . huge living room and family room plus spacious bedrooms generous bathrooms with bathtubs great school district oliveira elementary thornton junior high american high few mins drive to bart and lucky and supermarkets spacious two car garage easy access to fwy and dumbarton bridge . deposit one month rent lease one year minimum longer preferred no pet or smoking . no section . no sub leasing . tenant pay all utilities . gardening is included in the rent a bedroom single family house for rent in central fremont', 'raw_data': {'uuid': 'C_f82ff19f74094889873c8acc58ee582f', 'scenario': {'category': 'housing', 'uuid': 'S_Jrg93XX4AYzY9IQ7', 'post_id': '6131652108', 'kbs': [{'personal': {'Bottomline': None, 'Role': 'buyer', 'Target': 1920}, 'item': {'Category': 'housing', 'Images': [], 'Price': 3200, 'Description': [\"This is a single family house in an excellent condition. It will be available for move-in on July 2017. It's in a very nice and quiet community with great school district. Email me for viewing or further information. Thanks.\", 'Huge living room and family room, plus 4 spacious bedrooms', '2 generous bathrooms with bathtubs', 'Great school district (Oliveira elementary, Thornton junior high, American high)', 'Few mins drive to Bart and Lucky and 99 Supermarkets'], 'Title': 'A 4 bedroom single family house for rent in central Fremont'}}, {'personal': {'Bottomline': None, 'Role': 'seller', 'Target': 3200}, 'item': {'Category': 'housing', 'Images': [], 'Price': 3200, 'Description': [\"This is a single family house in an excellent condition. It will be available for move-in on July 2017. It's in a very nice and quiet community with great school district. Email me for viewing or further information. Thanks.\", 'Huge living room and family room, plus 4 spacious bedrooms', '2 generous bathrooms with bathtubs', 'Great school district (Oliveira elementary, Thornton junior high, American high)', 'Few mins drive to Bart and Lucky and 99 Supermarkets', 'Spacious two-car garage', 'Easy access to FWY 880 and Dumbarton Bridge.', 'Deposit: one month rent', 'Lease: one year minimum, longer preferred', 'No pet or smoking. No section 8. No sub-leasing.', 'Tenant pay all utilities. Gardening is included in the rent'], 'Title': 'A 4 bedroom single family house for rent in central Fremont'}}], 'attributes': [{'entity': False, 'unique': False, 'value_type': 'role', 'multivalued': False, 'name': 'Role'}, {'entity': False, 'unique': False, 'value_type': 'price', 'multivalued': False, 'name': 'Bottomline'}, {'entity': False, 'unique': False, 'value_type': 'price', 'multivalued': False, 'name': 'Target'}, {'entity': False, 'unique': False, 'value_type': 'text', 'multivalued': False, 'name': 'Title'}, {'entity': False, 'unique': False, 'value_type': 'text', 'multivalued': False, 'name': 'Category'}, {'entity': False, 'unique': False, 'value_type': 'price', 'multivalued': False, 'name': 'Price'}, {'entity': False, 'unique': False, 'value_type': 'text', 'multivalued': False, 'name': 'Images'}, {'entity': False, 'unique': False, 'value_type': 'text', 'multivalued': False, 'name': 'Description'}], 'intersection': 1.0}, 'scenario_uuid': 'S_vjAzVfmyolx9HnUe', 'agents': {'1': 'human', '0': 'human'}, 'outcome': {'reward': 1, 'offer': {'price': 1900.0, 'sides': ''}, 'buyer_start_amt': 1850.0, 'seller_suggested_amt': 3200, 'accepted_amt': 1900.0}, 'events': [{'action': 'message', 'start_time': '1496341329.09', 'data': 'i would love to buy ', 'agent': 0, 'time': '1496341331.9', 'strategies': ['buyer_pos_sentiment', 'first_person_singular_count_buyer', 'hedge_count_buyer']}, {'action': 'message', 'start_time': '1496341336.5', 'data': \"sure! what's your price?\", 'agent': 1, 'time': '1496341357.74', 'strategies': ['seller_pos_sentiment', 'number_of_diff_dic_neg', 'assertive_count_seller', 'personal_concern_seller']}, {'action': 'message', 'start_time': '1496341369.21', 'data': 'im on a budget so i could do 1850', 'agent': 0, 'time': '1496341377.98', 'strategies': ['first_person_singular_count_buyer', 'buyer_propose', 'hedge_count_buyer']}, {'action': 'message', 'start_time': '1496341419.92', 'data': \"how about $1900 and i'll wave the deposit.\", 'agent': 1, 'time': '1496341433.02', 'strategies': ['first_person_singular_count_seller', 'number_of_diff_dic_neg', 'hedge_count_seller', 'personal_concern_seller']}, {'action': 'message', 'start_time': '1496341447.57', 'data': 'i will take it', 'agent': 0, 'time': '1496341449.28', 'strategies': ['first_person_singular_count_buyer', 'third_person_singular_buyer']}, {'action': 'message', 'start_time': '1496341453.2', 'data': 'great!', 'agent': 1, 'time': '1496341454.29', 'strategies': ['seller_pos_sentiment']}, {'action': 'offer', 'start_time': None, 'data': {'price': 1900.0, 'sides': ''}, 'agent': 1, 'time': '1496341458.26', 'strategies': []}, {'action': 'offer', 'start_time': None, 'data': {'price': 1900.0, 'sides': ''}, 'agent': 0, 'time': '1496341458.58', 'strategies': []}, {'action': 'accept', 'start_time': None, 'data': None, 'agent': 1, 'time': '1496341462.53', 'strategies': []}, {'action': 'accept', 'start_time': None, 'data': None, 'agent': 0, 'time': '1496341464.61', 'strategies': []}]}, 'strategies_vec': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0.]]), 'dial_acts_vec': array([14,  8,  2,  1,  4,  5, 13,  7,  7, 10, 10]), 'toks_space': [[2, 1, 3], [2, 94, 84, 277, 29, 103, 3], [2, 82, 169, 173, 192, 46, 278, 92, 3], [2, 279, 14, 98, 280, 281, 94, 182, 170, 282, 3], [2, 111, 112, 283, 55, 94, 117, 284, 15, 255, 17, 3], [2, 94, 93, 285, 89, 3], [2, 162, 169, 3], [2, 4, 3], [2, 4, 3], [2, 6, 3], [2, 6, 3]], 'toks_bert': [[101, 1026, 2707, 1028, 102], [101, 1045, 2052, 2293, 2000, 4965, 102], [101, 2469, 999, 2054, 1055, 2115, 3976, 1029, 102], [101, 10047, 2006, 1037, 5166, 2061, 1045, 2071, 2079, 1026, 3976, 1028, 1035, 1011, 1014, 1012, 1015, 102], [101, 2129, 2055, 1026, 3976, 1028, 1035, 1011, 1014, 1012, 1014, 1998, 1045, 2222, 4400, 1996, 12816, 1012, 102], [101, 1045, 2097, 2202, 2009, 102], [101, 2307, 999, 102], [101, 1026, 3749, 1028, 102], [101, 1026, 3749, 1028, 102], [101, 1026, 5138, 1028, 102], [101, 1026, 5138, 1028, 102]], 'scene_space': [209, 18, 70, 98, 210, 211, 212, 42, 22, 213, 214, 17, 89, 93, 183, 215, 60, 216, 42, 14, 217, 17, 89, 192, 42, 98, 218, 180, 55, 219, 220, 21, 162, 221, 222, 17, 223, 90, 60, 224, 139, 225, 226, 17, 227, 17, 228, 229, 230, 55, 211, 230, 231, 232, 233, 234, 235, 21, 236, 162, 221, 222, 237, 238, 239, 240, 241, 242, 241, 243, 244, 245, 29, 246, 55, 247, 55, 248, 232, 11, 76, 249, 250, 251, 29, 252, 55, 253, 254, 17, 255, 256, 257, 258, 259, 256, 260, 261, 262, 263, 39, 264, 139, 265, 17, 39, 266, 17, 39, 267, 268, 17, 269, 270, 137, 271, 17, 272, 70, 273, 42, 15, 258, 98, 274, 210, 211, 212, 60, 258, 42, 275, 276], 'scene_bert': [101, 3847, 2023, 2003, 1037, 2309, 2155, 2160, 1999, 2019, 6581, 4650, 1012, 2009, 2097, 2022, 2800, 2005, 2693, 1999, 2006, 2251, 1012, 2009, 1055, 1999, 1037, 2200, 3835, 1998, 4251, 2451, 2007, 2307, 2082, 2212, 1012, 10373, 2033, 2005, 10523, 2030, 2582, 2592, 1012, 4283, 1012, 4121, 2542, 2282, 1998, 2155, 2282, 4606, 22445, 18390, 12382, 28942, 2007, 7198, 28251, 2015, 2307, 2082, 2212, 24263, 4732, 14630, 3502, 2152, 2137, 2152, 2261, 8117, 2015, 3298, 2000, 12075, 1998, 5341, 1998, 26676, 22445, 2048, 2482, 7381, 3733, 3229, 2000, 1042, 18418, 1998, 12873, 8445, 2239, 2958, 1012, 12816, 2028, 3204, 9278, 10084, 2028, 2095, 6263, 2936, 6871, 2053, 9004, 2030, 9422, 1012, 2053, 2930, 1012, 2053, 4942, 26707, 1012, 16713, 3477, 2035, 16548, 1012, 21529, 2003, 2443, 1999, 1996, 9278, 1037, 5010, 2309, 2155, 2160, 2005, 9278, 1999, 2430, 22550, 102], 'ratio_bucket': 1}\n"
     ]
    }
   ],
   "source": [
    "for dtype in dtypes:\n",
    "    print (dtype)\n",
    "    if dtype == 'dev': dtype = 'valid'\n",
    "    for datapoint in strategy_vector_data[dtype]:\n",
    "        ratio = datapoint['ratio']\n",
    "        if   ratio <= max(bucket1): bucket = 0\n",
    "        elif ratio <= max(bucket2): bucket = 1\n",
    "        elif ratio <= max(bucket3): bucket = 2\n",
    "        elif ratio <= max(bucket4): bucket = 3\n",
    "        else:                       bucket = 4\n",
    "        datapoint['ratio_bucket'] = bucket\n",
    "print (strategy_vector_data['train'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(strategy_vector_data, open('../../../data/negotiation_data/data/strategy_vector_data_FULL.pkl', 'wb'))\n",
    "pickle.dump(strategy_vector_data, open('../../data/negotiation_data/strategy_vector_data_FULL_Yiheng.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<price>_-0.2\n",
      "<price>_0.0\n",
      "<price>_0.4\n",
      "<price>_0.3\n",
      "<price>_-0.1\n",
      "<price>_-0.0\n",
      "<price>_-0.3\n",
      "<price>_0.6\n",
      "<price>_0.5\n",
      "<price>_-1.5\n",
      "<price>_0.7\n",
      "<price>_-0.6\n",
      "<price>_-2.0\n",
      "<price>_-1.2\n",
      "<price>_0.8\n",
      "<price>_-0.7\n",
      "<price>_2.0\n",
      "<price>_0.2\n",
      "<price>_-1.9\n",
      "<price>_-0.4\n",
      "<price>_1.1\n",
      "<price>_0.9\n",
      "<price>_1.0\n",
      "<price>_-0.5\n",
      "<price>_0.1\n",
      "<price>_1.8\n",
      "<price>_1.2\n",
      "<price>_-1.0\n",
      "<price>_1.4\n",
      "<price>_-0.8\n",
      "<price>_-0.9\n",
      "<price>_-1.1\n",
      "<price>_-1.3\n",
      "<price>_1.5\n",
      "<price>_-1.4\n",
      "<price>_1.6\n",
      "<price>_1.7\n",
      "<price>_1.3\n",
      "<price>_-1.7\n",
      "<price>_1.9\n",
      "<price>_-1.6\n",
      "<price>_-1.8\n"
     ]
    }
   ],
   "source": [
    "for w in strategy_vector_data['word2id']:\n",
    "    if '<price' in w:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
